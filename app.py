"""
å•†å“åœ¨åº« Ã— GA4å£²ä¸Š åˆ†æã‚¢ãƒ—ãƒª
- å•†å“ãƒã‚¹ã‚¿CSVï¼ˆåœ¨åº«ãƒ»è‰²ãƒ»ç”»åƒãªã©ï¼‰ã¨GA4å£²ä¸ŠCSVã‚’çªãåˆã‚ã›
- ãƒ–ãƒ©ãƒ³ãƒ‰åˆ¥ã«ã€Œåœ¨åº«éå¤šÃ—ä½å£²ä¸Šã€ãªã©ã®å•†å“ã‚’åˆ†æ
- å•†å“ç”»åƒä»˜ãã§ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
"""

import os
import re
from datetime import datetime

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç”¨ï¼‰
from dotenv import load_dotenv
load_dotenv()

from flask import Flask, render_template, request, redirect, url_for, flash, jsonify, session
from functools import wraps
import pandas as pd
from werkzeug.utils import secure_filename

# ============================================
# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
# ============================================
# ãƒ¡ãƒ¢ãƒªä¸Šã®ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥
password_cache = {
    'admin': None,
    'brands': {}
}

def get_default_passwords():
    """ç’°å¢ƒå¤‰æ•°ã‹ã‚‰åˆæœŸãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—"""
    return {
        'admin': os.environ.get('ADMIN_PASSWORD', 'admin898989'),
        'brands': {
            'rady': os.environ.get('BRAND_PASSWORD_RADY', 'rady2025'),
            'cherimi': os.environ.get('BRAND_PASSWORD_CHERIMI', 'cherimi2025'),
            'michellmacaron': os.environ.get('BRAND_PASSWORD_MICHELLMACARON', 'mm2025'),
            'solni': os.environ.get('BRAND_PASSWORD_SOLNI', 'solni2025'),
        }
    }

def init_passwords():
    """ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’åˆæœŸåŒ–ï¼ˆR2ã‹ã‚‰èª­ã¿è¾¼ã¿ã€ãªã‘ã‚Œã°ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ï¼‰"""
    global password_cache
    
    # R2ã‹ã‚‰èª­ã¿è¾¼ã¿ã‚’è©¦è¡Œ
    if r2_load_passwords:
        r2_passwords = r2_load_passwords()
        if r2_passwords:
            password_cache = r2_passwords
            print("[OK] Loaded passwords from R2")
            return
    
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰åˆæœŸå€¤ã‚’è¨­å®š
    password_cache = get_default_passwords()
    print("[OK] Initialized passwords from environment variables")

def check_password(entered_password):
    """
    ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ãªãƒ–ãƒ©ãƒ³ãƒ‰ã‚’è¿”ã™
    Returns: {'is_admin': bool, 'brands': list} or None
    """
    global password_cache
    
    # ç®¡ç†è€…ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
    if entered_password == password_cache.get('admin'):
        return {'is_admin': True, 'brands': BRANDS}
    
    # ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
    for brand, pwd in password_cache.get('brands', {}).items():
        if entered_password == pwd:
            return {'is_admin': False, 'brands': [brand]}
    
    return None

def update_password(password_type, brand_key=None, new_password=None):
    """ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’æ›´æ–°ï¼ˆR2ã«ã‚‚ä¿å­˜ï¼‰"""
    global password_cache
    
    if password_type == 'admin':
        password_cache['admin'] = new_password
    elif password_type == 'brand' and brand_key:
        if 'brands' not in password_cache:
            password_cache['brands'] = {}
        password_cache['brands'][brand_key.lower()] = new_password
    
    # R2ã«ä¿å­˜
    if r2_save_passwords:
        r2_save_passwords(password_cache)
    
    return True

def login_required(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not session.get('logged_in'):
            return redirect(url_for('login'))
        return f(*args, **kwargs)
    return decorated_function

def admin_required(f):
    """ç®¡ç†è€…å°‚ç”¨ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not session.get('logged_in'):
            return redirect(url_for('login'))
        if not session.get('is_admin'):
            flash('ã“ã®æ©Ÿèƒ½ã¯ç®¡ç†è€…ã®ã¿åˆ©ç”¨å¯èƒ½ã§ã™', 'error')
            return redirect(url_for('index'))
        return f(*args, **kwargs)
    return decorated_function

def can_access_brand(brand_name):
    """ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§æŒ‡å®šãƒ–ãƒ©ãƒ³ãƒ‰ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã‹"""
    if session.get('is_admin'):
        return True
    accessible = session.get('accessible_brands', [])
    # ãƒ–ãƒ©ãƒ³ãƒ‰åã®æ­£è¦åŒ–ï¼ˆå¤§æ–‡å­—å°æ–‡å­—ç„¡è¦–ï¼‰
    brand_lower = brand_name.lower()
    return any(b.lower() == brand_lower for b in accessible)

# R2ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸é€£æº
try:
    from storage import (
        download_product_master, upload_product_master, get_product_master_info, 
        is_r2_enabled, save_ga4_data, get_latest_ga4_data,
        save_passwords as r2_save_passwords, load_passwords as r2_load_passwords
    )
except ImportError:
    is_r2_enabled = lambda: False
    download_product_master = None
    save_ga4_data = None
    get_latest_ga4_data = None
    upload_product_master = None
    get_product_master_info = None
    r2_save_passwords = None
    r2_load_passwords = None

# GA4 APIé€£æº
try:
    from ga4_api import (
        is_ga4_configured, get_configured_brands, 
        fetch_yesterday_data, fetch_weekly_data, fetch_all_brands_data
    )
except ImportError:
    is_ga4_configured = lambda: False
    get_configured_brands = lambda: []
    fetch_yesterday_data = None
    fetch_weekly_data = None
    fetch_all_brands_data = None

app = Flask(__name__)
app.secret_key = os.environ.get('SECRET_KEY', 'dev-secret-key-change-in-production')
app.config['UPLOAD_FOLDER'] = os.path.join(os.path.dirname(__file__), 'uploads')
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MB

# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆã‚¢ï¼ˆæœ¬ç•ªã§ã¯DBã‚’ä½¿ç”¨ï¼‰
data_store = {
    'product_master': None,
    'product_master_info': None,  # R2ã‹ã‚‰ã®æƒ…å ±
    'ga_sales': {},  # ãƒ–ãƒ©ãƒ³ãƒ‰åˆ¥ã«ä¿å­˜ {'rady': {'data': df, 'period': {...}}, ...}
    'ga_sales_previous': {},  # å‰æœŸé–“ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¯”è¼ƒç”¨ï¼‰
    'channel_data': {},  # ãƒãƒ£ãƒãƒ«åˆ¥ãƒ‡ãƒ¼ã‚¿ {'rady': df, ...}
    'merged_data': None,
    'merged_data_previous': None,  # å‰æœŸé–“ã®ãƒãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿
    'current_period': 'yesterday',  # ç¾åœ¨è¡¨ç¤ºä¸­ã®æœŸé–“
    # æœŸé–“åˆ¥ãƒ‡ãƒ¼ã‚¿ï¼ˆè‡ªå‹•æ›´æ–°ç”¨ï¼‰
    'periods_data': {
        'yesterday': {'ga_sales': {}, 'ga_sales_previous': {}, 'channel_data': {}, 'merged_data': None, 'merged_data_previous': None},
        'weekly': {'ga_sales': {}, 'ga_sales_previous': {}, 'channel_data': {}, 'merged_data': None, 'merged_data_previous': None},
        '3days': {'ga_sales': {}, 'ga_sales_previous': {}, 'channel_data': {}, 'merged_data': None, 'merged_data_previous': None},
    }
}

# ç™»éŒ²æ¸ˆã¿ãƒ–ãƒ©ãƒ³ãƒ‰ä¸€è¦§
BRANDS = ['rady', 'cherimi', 'michellmacaron', 'solni']


def process_product_master_df(df):
    """å•†å“ãƒã‚¹ã‚¿DataFrameã‚’å‡¦ç†"""
    # å¿…è¦ãªã‚«ãƒ©ãƒ ã‚’æŠ½å‡ºãƒ»ãƒªãƒãƒ¼ãƒ 
    col_map = {
        'SKUå•†å“ID': 'sku_id',
        'å•†å“IDï¼ˆå‹å˜ä½ï¼‰': 'product_class_id',
        'ãƒ–ãƒ©ãƒ³ãƒ‰å': 'brand',
        'å•†å“å': 'product_name',
        'ã‚«ãƒ©ãƒ¼å': 'color_name',
        'ã‚«ãƒ©ãƒ¼ã‚¿ã‚°': 'color_tag',
        'ã‚µã‚¤ã‚ºå': 'size',
        'è²©å£²ä¾¡æ ¼': 'price',
        'WEBåœ¨åº«': 'web_stock',
        'èª¿æ•´åœ¨åº«': 'adjust_stock',
        'è¦‹è¾¼ã¿åœ¨åº«': 'expected_stock',
        'å•†å“ãƒšãƒ¼ã‚¸URL': 'product_url',
        'å•†å“ç”»åƒURL': 'image_url',
        'å…¬é–‹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': 'publish_status',
        'è²©å£²ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': 'sales_status',
    }
    # å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã®ã¿ãƒªãƒãƒ¼ãƒ 
    existing_cols = {k: v for k, v in col_map.items() if k in df.columns}
    df = df.rename(columns=existing_cols)
    
    # åœ¨åº«åˆè¨ˆã‚’è¨ˆç®—
    stock_cols = ['web_stock', 'adjust_stock', 'expected_stock']
    for col in stock_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)
    
    df['total_stock'] = df[['web_stock', 'adjust_stock', 'expected_stock']].sum(axis=1) if all(c in df.columns for c in stock_cols) else 0
    
    return df


def load_product_master(filepath):
    """å•†å“ãƒã‚¹ã‚¿CSVã‚’èª­ã¿è¾¼ã¿ï¼ˆcp932/utf-8å¯¾å¿œï¼‰"""
    for enc in ['cp932', 'utf-8', 'utf-8-sig']:
        try:
            df = pd.read_csv(filepath, encoding=enc)
            return process_product_master_df(df)
        except Exception:
            continue
    raise ValueError("CSVã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")


def parse_ga_period(lines):
    """GA4 CSVã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰æœŸé–“æƒ…å ±ã‚’æŠ½å‡º"""
    period = {
        'start_date': None,
        'end_date': None,
        'property': None,
        'days': 0,
        'period_type': 'unknown'  # daily, weekly, monthly, custom
    }
    
    for line in lines[:15]:  # æœ€åˆã®15è¡Œã ã‘ãƒã‚§ãƒƒã‚¯
        line = line.strip()
        
        # Start date: 20251127 å½¢å¼
        if 'Start date:' in line:
            match = re.search(r'Start date:\s*(\d{8})', line)
            if match:
                date_str = match.group(1)
                try:
                    period['start_date'] = datetime.strptime(date_str, '%Y%m%d')
                except:
                    pass
        
        # End date: 20251128 å½¢å¼
        if 'End date:' in line:
            match = re.search(r'End date:\s*(\d{8})', line)
            if match:
                date_str = match.group(1)
                try:
                    period['end_date'] = datetime.strptime(date_str, '%Y%m%d')
                except:
                    pass
        
        # Propertyå
        if 'Property:' in line:
            match = re.search(r'Property:\s*(.+)', line)
            if match:
                period['property'] = match.group(1).strip()
    
    # æ—¥æ•°è¨ˆç®—ã¨æœŸé–“ã‚¿ã‚¤ãƒ—åˆ¤å®š
    if period['start_date'] and period['end_date']:
        delta = period['end_date'] - period['start_date']
        period['days'] = delta.days + 1  # ä¸¡ç«¯å«ã‚€
        
        if period['days'] == 1:
            period['period_type'] = 'daily'
        elif period['days'] == 7:
            period['period_type'] = 'weekly'
        elif 28 <= period['days'] <= 31:
            period['period_type'] = 'monthly'
        else:
            period['period_type'] = 'custom'
    
    return period


def load_ga_sales(filepath):
    """GA4å£²ä¸ŠCSVã‚’èª­ã¿è¾¼ã¿ã€æœŸé–“æƒ…å ±ã‚‚è¿”ã™"""
    for enc in ['utf-8', 'utf-8-sig', 'cp932']:
        try:
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆGA4ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå½¢å¼å¯¾å¿œï¼‰
            with open(filepath, 'r', encoding=enc) as f:
                lines = f.readlines()
            
            # æœŸé–“æƒ…å ±ã‚’æŠ½å‡º
            period = parse_ga_period(lines)
            
            # ãƒ‡ãƒ¼ã‚¿é–‹å§‹è¡Œã‚’æ¢ã™ï¼ˆ#ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            header_idx = 0
            for i, line in enumerate(lines):
                # #ã§å§‹ã¾ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                if line.strip().startswith('#'):
                    continue
                # Item nameã¾ãŸã¯Item IDã‚’å«ã‚€ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’æ¢ã™
                if 'Item name' in line or 'Item ID' in line:
                    header_idx = i
                    break
            
            df = pd.read_csv(filepath, encoding=enc, skiprows=header_idx)
            
            # ã‚«ãƒ©ãƒ åã‚’æ­£è¦åŒ–
            col_map = {
                'Item name': 'item_name',
                'Item ID': 'sku_id',
                'Items viewed': 'views',
                'Items added to cart': 'add_to_cart',
                'Items purchased': 'purchases',
                'Item revenue': 'revenue',
            }
            existing_cols = {k: v for k, v in col_map.items() if k in df.columns}
            df = df.rename(columns=existing_cols)
            
            # sku_idãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
            if 'sku_id' not in df.columns:
                raise ValueError("Item ID column not found")
            
            # æ•°å€¤å¤‰æ›
            for col in ['views', 'add_to_cart', 'purchases', 'revenue']:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
            
            return {'data': df, 'period': period}
        except Exception as e:
            continue
    raise ValueError("GA4 CSVã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")


def get_analysis_period():
    """ç¾åœ¨ã®åˆ†ææœŸé–“æƒ…å ±ã‚’å–å¾—"""
    ga_dict = data_store['ga_sales']
    if not ga_dict:
        return None
    
    # å…¨ãƒ–ãƒ©ãƒ³ãƒ‰ã®æœŸé–“ã‚’é›†ç´„
    all_periods = []
    for brand, ga_info in ga_dict.items():
        if ga_info and 'period' in ga_info:
            period = ga_info['period']
            period['brand'] = brand
            all_periods.append(period)
    
    if not all_periods:
        return None
    
    # å…¨ä½“ã®æœŸé–“ã‚’è¨ˆç®—
    start_dates = [p['start_date'] for p in all_periods if p['start_date']]
    end_dates = [p['end_date'] for p in all_periods if p['end_date']]
    
    overall = {
        'brands': all_periods,
        'min_start': min(start_dates) if start_dates else None,
        'max_end': max(end_dates) if end_dates else None,
    }
    
    if overall['min_start'] and overall['max_end']:
        delta = overall['max_end'] - overall['min_start']
        overall['total_days'] = delta.days + 1
    else:
        overall['total_days'] = 0
    
    return overall


def switch_period_data(period_type):
    """
    æŒ‡å®šã—ãŸæœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¡ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ã«ã‚³ãƒ”ãƒ¼
    """
    if period_type not in data_store['periods_data']:
        return False
    
    period_data = data_store['periods_data'][period_type]
    
    # ãƒ¡ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ã«ã‚³ãƒ”ãƒ¼
    data_store['ga_sales'] = period_data['ga_sales'].copy()
    data_store['ga_sales_previous'] = period_data['ga_sales_previous'].copy()
    data_store['channel_data'] = period_data['channel_data'].copy()
    data_store['merged_data'] = period_data['merged_data']
    data_store['merged_data_previous'] = period_data['merged_data_previous']
    data_store['current_period'] = period_type
    
    return True


def merge_and_analyze_for_period(period_type):
    """
    æŒ‡å®šã—ãŸæœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã§åˆ†æã‚’å®Ÿè¡Œã—ã€çµæœã‚’æœŸé–“åˆ¥ã‚¹ãƒˆã‚¢ã«ä¿å­˜
    """
    if period_type not in data_store['periods_data']:
        return None
    
    period_data = data_store['periods_data'][period_type]
    pm = data_store['product_master']
    ga_dict = period_data['ga_sales']
    ga_dict_prev = period_data['ga_sales_previous']
    
    if pm is None or not ga_dict:
        return None
    
    # ç¾åœ¨æœŸé–“ã®ãƒãƒ¼ã‚¸
    ga_list = [info['data'] for info in ga_dict.values() if info and 'data' in info and len(info['data']) > 0]
    if not ga_list:
        return None
    
    ga = pd.concat(ga_list, ignore_index=True)
    ga = ga.groupby('sku_id').agg({
        'item_name': 'first',
        'views': 'sum',
        'add_to_cart': 'sum',
        'purchases': 'sum',
        'revenue': 'sum',
    }).reset_index()
    
    merged = pm.merge(ga, on='sku_id', how='left')
    merged['views'] = merged['views'].fillna(0).astype(int)
    merged['add_to_cart'] = merged['add_to_cart'].fillna(0).astype(int)
    merged['purchases'] = merged['purchases'].fillna(0).astype(int)
    merged['revenue'] = merged['revenue'].fillna(0)
    
    period_data['merged_data'] = merged
    
    # å‰æœŸé–“ã®ãƒãƒ¼ã‚¸
    if ga_dict_prev:
        ga_list_prev = [info['data'] for info in ga_dict_prev.values() if info and 'data' in info and len(info['data']) > 0]
        if ga_list_prev:
            ga_prev = pd.concat(ga_list_prev, ignore_index=True)
            ga_prev = ga_prev.groupby('sku_id').agg({
                'item_name': 'first',
                'views': 'sum',
                'add_to_cart': 'sum',
                'purchases': 'sum',
                'revenue': 'sum',
            }).reset_index()
            merged_prev = pm.merge(ga_prev, on='sku_id', how='left')
            merged_prev['views'] = merged_prev['views'].fillna(0).astype(int)
            merged_prev['add_to_cart'] = merged_prev['add_to_cart'].fillna(0).astype(int)
            merged_prev['purchases'] = merged_prev['purchases'].fillna(0).astype(int)
            merged_prev['revenue'] = merged_prev['revenue'].fillna(0)
            period_data['merged_data_previous'] = merged_prev
    
    return merged


def merge_and_analyze():
    """å•†å“ãƒã‚¹ã‚¿ã¨GAå£²ä¸Šã‚’çªãåˆã‚ã›ã¦åˆ†æ"""
    pm = data_store['product_master']
    ga_dict = data_store['ga_sales']
    
    if pm is None or not ga_dict:
        return None
    
    # å…¨ãƒ–ãƒ©ãƒ³ãƒ‰ã®GAãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    ga_list = [info['data'] for info in ga_dict.values() if info and 'data' in info and len(info['data']) > 0]
    if not ga_list:
        return None
    
    ga = pd.concat(ga_list, ignore_index=True)
    
    # åŒã˜SKUãŒè¤‡æ•°ãƒ–ãƒ©ãƒ³ãƒ‰ã«ã‚ã‚‹å ´åˆã¯åˆç®—
    ga = ga.groupby('sku_id').agg({
        'item_name': 'first',
        'views': 'sum',
        'add_to_cart': 'sum',
        'purchases': 'sum',
        'revenue': 'sum',
    }).reset_index()
    
    # SKU IDã§çµåˆ
    merged = pm.merge(ga, on='sku_id', how='left')
    
    # å•†å“ãƒšãƒ¼ã‚¸URLã‚’è‡ªå‹•ç”Ÿæˆï¼ˆç©ºã®å ´åˆï¼‰
    def generate_product_url(row):
        if pd.notna(row.get('product_url')) and str(row.get('product_url')).strip():
            return row['product_url']
        
        brand_raw = str(row.get('brand', '')).lower().replace(' ', '').replace('_', '')
        brand_slug = ''
        if 'rady' in brand_raw:
            brand_slug = 'rady'
        elif 'cherimi' in brand_raw:
            brand_slug = 'cherimi'
        elif 'michell' in brand_raw or 'macaron' in brand_raw:
            brand_slug = 'michellmacaron'
        elif 'solni' in brand_raw:
            brand_slug = 'solni'
        
        if brand_slug and row.get('sku_id'):
            return f"https://mycolor.jp/{brand_slug}/item/{row['sku_id']}"
        return ''
    
    merged['product_url'] = merged.apply(generate_product_url, axis=1)
    
    # æ¬ æå€¤ã‚’0åŸ‹ã‚
    for col in ['views', 'add_to_cart', 'purchases', 'revenue']:
        if col in merged.columns:
            merged[col] = merged[col].fillna(0)
    
    # åˆ†ææŒ‡æ¨™ã‚’è¿½åŠ 
    # CVRï¼ˆé–²è¦§â†’è³¼å…¥ç‡ï¼‰
    merged['cvr'] = merged.apply(
        lambda x: (x['purchases'] / x['views'] * 100) if x['views'] > 0 else 0, axis=1
    )
    
    # ã‚«ãƒ¼ãƒˆè¿½åŠ ç‡
    merged['cart_rate'] = merged.apply(
        lambda x: (x['add_to_cart'] / x['views'] * 100) if x['views'] > 0 else 0, axis=1
    )
    
    # åœ¨åº«åŠ¹ç‡ã‚¹ã‚³ã‚¢ï¼ˆå£²ä¸ŠÃ·åœ¨åº«ã€é«˜ã„ã»ã©åŠ¹ç‡çš„ï¼‰
    merged['stock_efficiency'] = merged.apply(
        lambda x: x['revenue'] / x['total_stock'] if x['total_stock'] > 0 else 0, axis=1
    )
    
    # å•é¡Œãƒ•ãƒ©ã‚°: åœ¨åº«å¤šã„ Ã— å£²ä¸Šå°‘ãªã„
    stock_threshold = merged['total_stock'].quantile(0.7)  # ä¸Šä½30%ã®åœ¨åº«
    revenue_threshold = merged['revenue'].quantile(0.3)    # ä¸‹ä½30%ã®å£²ä¸Š
    
    merged['is_problem'] = (merged['total_stock'] >= stock_threshold) & (merged['revenue'] <= revenue_threshold)
    
    # æ©Ÿä¼šæå¤±ãƒ•ãƒ©ã‚°: é–²è¦§å¤šã„ Ã— åœ¨åº«å°‘ãªã„ Ã— è³¼å…¥å°‘ãªã„
    views_threshold = merged['views'].quantile(0.7)
    merged['is_opportunity'] = (merged['views'] >= views_threshold) & (merged['total_stock'] <= 5) & (merged['purchases'] < merged['views'] * 0.05)
    
    # Regalectã‚’é™¤å¤–
    merged = merged[merged['brand'] != 'Regalect']
    
    # å‰æœŸé–“ãƒ‡ãƒ¼ã‚¿ã¨ã®æ¯”è¼ƒï¼ˆãƒ‡ãƒ«ã‚¿è¨ˆç®—ï¼‰
    ga_prev_dict = data_store.get('ga_sales_previous', {})
    if ga_prev_dict:
        ga_prev_list = [info['data'] for info in ga_prev_dict.values() if info and 'data' in info and len(info['data']) > 0]
        if ga_prev_list:
            ga_prev = pd.concat(ga_prev_list, ignore_index=True)
            ga_prev = ga_prev.groupby('sku_id').agg({
                'views': 'sum',
                'add_to_cart': 'sum',
                'purchases': 'sum',
                'revenue': 'sum',
            }).reset_index()
            ga_prev.columns = ['sku_id', 'prev_views', 'prev_add_to_cart', 'prev_purchases', 'prev_revenue']
            
            # å‰æœŸé–“ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
            merged = merged.merge(ga_prev, on='sku_id', how='left')
            
            # ãƒ‡ãƒ«ã‚¿è¨ˆç®—
            for col in ['views', 'add_to_cart', 'purchases', 'revenue']:
                prev_col = f'prev_{col}'
                delta_col = f'delta_{col}'
                pct_col = f'delta_{col}_pct'
                
                merged[prev_col] = merged[prev_col].fillna(0)
                merged[delta_col] = merged[col] - merged[prev_col]
                merged[pct_col] = merged.apply(
                    lambda x: ((x[col] - x[prev_col]) / x[prev_col] * 100) if x[prev_col] > 0 else (100 if x[col] > 0 else 0),
                    axis=1
                )
            
            # CVRã®ãƒ‡ãƒ«ã‚¿
            merged['prev_cvr'] = merged.apply(
                lambda x: (x['prev_purchases'] / x['prev_views'] * 100) if x['prev_views'] > 0 else 0, axis=1
            )
            merged['delta_cvr'] = merged['cvr'] - merged['prev_cvr']
            
            print(f"[OK] Calculated deltas for {len(merged)} items")
    
    data_store['merged_data'] = merged
    return merged


def get_brand_summary():
    """ãƒ–ãƒ©ãƒ³ãƒ‰åˆ¥ã‚µãƒãƒªãƒ¼ã‚’å–å¾—ï¼ˆå‰æœŸé–“æ¯”è¼ƒä»˜ãï¼‰"""
    df = data_store['merged_data']
    if df is None:
        return None
    
    # Regalectã‚’é™¤å¤–
    df = df[df['brand'] != 'Regalect']
    
    # é›†è¨ˆã‚«ãƒ©ãƒ ã®æº–å‚™
    agg_dict = {
        'sku_id': 'count',
        'total_stock': 'sum',
        'views': 'sum',
        'add_to_cart': 'sum',
        'purchases': 'sum',
        'revenue': 'sum',
        'is_problem': 'sum',
        'is_opportunity': 'sum',
    }
    
    # å‰æœŸé–“ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°è¿½åŠ 
    has_prev = 'prev_revenue' in df.columns
    if has_prev:
        agg_dict['prev_revenue'] = 'sum'
        agg_dict['prev_views'] = 'sum'
        agg_dict['prev_add_to_cart'] = 'sum'
        agg_dict['prev_purchases'] = 'sum'
    
    summary = df.groupby('brand').agg(agg_dict).reset_index()
    
    # ã‚«ãƒ©ãƒ åã‚’å¤‰æ›´
    base_cols = ['brand', 'sku_count', 'total_stock', 'total_views', 
                 'total_add_to_cart', 'total_purchases', 'total_revenue',
                 'problem_count', 'opportunity_count']
    if has_prev:
        base_cols.extend(['prev_total_revenue', 'prev_total_views', 'prev_total_add_to_cart', 'prev_total_purchases'])
    summary.columns = base_cols
    
    # CVRè¨ˆç®—
    summary['overall_cvr'] = summary.apply(
        lambda x: (x['total_purchases'] / x['total_views'] * 100) if x['total_views'] > 0 else 0, axis=1
    )
    
    # ãƒ‡ãƒ«ã‚¿è¨ˆç®—ï¼ˆå‰æœŸé–“ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆï¼‰
    if has_prev:
        # å£²ä¸Šãƒ‡ãƒ«ã‚¿
        summary['delta_revenue'] = summary['total_revenue'] - summary['prev_total_revenue']
        summary['delta_revenue_pct'] = summary.apply(
            lambda x: ((x['total_revenue'] - x['prev_total_revenue']) / x['prev_total_revenue'] * 100) 
                      if x['prev_total_revenue'] > 0 else 0, axis=1
        )
        # PVãƒ‡ãƒ«ã‚¿
        summary['delta_views'] = summary['total_views'] - summary['prev_total_views']
        summary['delta_views_pct'] = summary.apply(
            lambda x: ((x['total_views'] - x['prev_total_views']) / x['prev_total_views'] * 100) 
                      if x['prev_total_views'] > 0 else 0, axis=1
        )
        # è³¼å…¥ãƒ‡ãƒ«ã‚¿
        summary['delta_purchases'] = summary['total_purchases'] - summary['prev_total_purchases']
        summary['delta_purchases_pct'] = summary.apply(
            lambda x: ((x['total_purchases'] - x['prev_total_purchases']) / x['prev_total_purchases'] * 100) 
                      if x['prev_total_purchases'] > 0 else 0, axis=1
        )
        # ã‚«ãƒ¼ãƒˆè¿½åŠ ãƒ‡ãƒ«ã‚¿
        summary['delta_add_to_cart'] = summary['total_add_to_cart'] - summary['prev_total_add_to_cart']
        summary['delta_add_to_cart_pct'] = summary.apply(
            lambda x: ((x['total_add_to_cart'] - x['prev_total_add_to_cart']) / x['prev_total_add_to_cart'] * 100) 
                      if x['prev_total_add_to_cart'] > 0 else 0, axis=1
        )
    
    return summary.to_dict('records')


def get_problem_products(brand=None, limit=50):
    """å•é¡Œå•†å“ï¼ˆåœ¨åº«éå¤šÃ—ä½å£²ä¸Šï¼‰ã‚’å–å¾—"""
    df = data_store['merged_data']
    if df is None:
        return []
    
    filtered = df[df['is_problem'] == True].copy()
    if brand and brand != 'all':
        filtered = filtered[filtered['brand'] == brand]
    
    filtered = filtered.sort_values('total_stock', ascending=False).head(limit)
    return filtered.to_dict('records')


def get_opportunity_products(brand=None, limit=50):
    """æ©Ÿä¼šæå¤±å•†å“ï¼ˆé–²è¦§å¤šÃ—åœ¨åº«åˆ‡ã‚Œï¼‰ã‚’å–å¾—"""
    df = data_store['merged_data']
    if df is None:
        return []
    
    filtered = df[df['is_opportunity'] == True].copy()
    if brand and brand != 'all':
        filtered = filtered[filtered['brand'] == brand]
    
    filtered = filtered.sort_values('views', ascending=False).head(limit)
    return filtered.to_dict('records')


def get_top_performers(brand=None, limit=30):
    """å£²ä¸Šä¸Šä½å•†å“ã‚’å–å¾—ï¼ˆã‚«ãƒ©ãƒ¼/ã‚µã‚¤ã‚ºåˆ¥ï¼‰"""
    df = data_store['merged_data']
    if df is None:
        return []
    
    filtered = df.copy()
    if brand and brand != 'all':
        filtered = filtered[filtered['brand'] == brand]
    
    filtered = filtered.sort_values('revenue', ascending=False).head(limit)
    return filtered.to_dict('records')


def get_anomalies(brand=None, limit=20):
    """
    ç•°å¸¸å€¤æ¤œå‡ºï¼šæ€¥ä¸Šæ˜‡å•†å“ã¨è¦æ³¨æ„å•†å“ã‚’å–å¾—
    - æ€¥ä¸Šæ˜‡: PVã¾ãŸã¯è³¼å…¥ãŒå‰æœŸæ¯”+50%ä»¥ä¸Šã‹ã¤ä¸€å®šä»¥ä¸Šã®å®Ÿç¸¾
    - è¦æ³¨æ„: åœ¨åº«ã‚ã‚‹ã®ã«PVã¾ãŸã¯è³¼å…¥ãŒå‰æœŸæ¯”-30%ä»¥ä¸Š
    """
    df = data_store['merged_data']
    if df is None:
        return {'rising': [], 'warning': []}
    
    # ãƒ‡ãƒ«ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒãªã‘ã‚Œã°ç©ºã‚’è¿”ã™
    if 'delta_views_pct' not in df.columns:
        return {'rising': [], 'warning': []}
    
    filtered = df.copy()
    if brand and brand != 'all':
        filtered = filtered[filtered['brand'] == brand]
    
    # ğŸ”¥ æ€¥ä¸Šæ˜‡å•†å“
    # æ¡ä»¶: (PV+50%ä»¥ä¸Š AND ä»ŠæœŸPV>=50) OR (è³¼å…¥+50%ä»¥ä¸Š AND ä»ŠæœŸè³¼å…¥>=3)
    rising_condition = (
        ((filtered['delta_views_pct'] >= 50) & (filtered['views'] >= 50)) |
        ((filtered['delta_purchases_pct'] >= 50) & (filtered['purchases'] >= 3))
    )
    rising = filtered[rising_condition].copy()
    
    # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆä¸Šæ˜‡ç‡ã®é«˜ã„é †ï¼‰
    rising['rise_score'] = (
        rising['delta_views_pct'].fillna(0) * 0.3 +
        rising['delta_purchases_pct'].fillna(0) * 0.5 +
        rising['delta_revenue_pct'].fillna(0) * 0.2
    )
    rising = rising.sort_values('rise_score', ascending=False).head(limit)
    
    # âš ï¸ è¦æ³¨æ„å•†å“ï¼ˆåœ¨åº«ã‚ã‚‹ã®ã«è½ã¡ã¦ã„ã‚‹ï¼‰
    # æ¡ä»¶: åœ¨åº«>0 AND ((PV-30%ä»¥ä¸Š AND å‰æœŸPV>=30) OR (è³¼å…¥-30%ä»¥ä¸Š AND å‰æœŸè³¼å…¥>=2))
    warning_condition = (
        (filtered['total_stock'] > 0) &
        (
            ((filtered['delta_views_pct'] <= -30) & (filtered['prev_views'] >= 30)) |
            ((filtered['delta_purchases_pct'] <= -30) & (filtered['prev_purchases'] >= 2))
        )
    )
    warning = filtered[warning_condition].copy()
    
    # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆä¸‹è½ç‡ã®å¤§ãã„é †ã€ãƒã‚¤ãƒŠã‚¹ãªã®ã§å°ã•ã„æ–¹ãŒæ‚ªã„ï¼‰
    warning['warn_score'] = (
        warning['delta_views_pct'].fillna(0) * 0.3 +
        warning['delta_purchases_pct'].fillna(0) * 0.5 +
        warning['delta_revenue_pct'].fillna(0) * 0.2
    )
    warning = warning.sort_values('warn_score', ascending=True).head(limit)
    
    # å¿…è¦ãªã‚«ãƒ©ãƒ ã ã‘æŠ½å‡ºã—ã¦è¾æ›¸ã«å¤‰æ›
    cols = ['sku_id', 'brand', 'product_name', 'color_name', 'size', 'image_url', 'product_url',
            'total_stock', 'views', 'prev_views', 'delta_views', 'delta_views_pct',
            'purchases', 'prev_purchases', 'delta_purchases', 'delta_purchases_pct',
            'revenue', 'prev_revenue', 'delta_revenue', 'delta_revenue_pct', 'cvr']
    
    def safe_to_dict(dataframe):
        result = []
        for _, row in dataframe.iterrows():
            item = {}
            for col in cols:
                if col in row.index:
                    val = row[col]
                    if pd.isna(val):
                        val = 0 if col not in ['sku_id', 'brand', 'product_name', 'color_name', 'size', 'image_url', 'product_url'] else ''
                    item[col] = val
                else:
                    item[col] = 0 if col not in ['sku_id', 'brand', 'product_name', 'color_name', 'size', 'image_url', 'product_url'] else ''
            result.append(item)
        return result
    
    return {
        'rising': safe_to_dict(rising),
        'warning': safe_to_dict(warning)
    }


def process_channel_data(channel_info):
    """
    ãƒãƒ£ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢ï¼ˆæ—¥æœ¬èªåŒ–ã€æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ä»˜ãï¼‰
    """
    from ga4_api import translate_channel_name, translate_source_name
    
    if not channel_info or 'current' not in channel_info:
        return []
    
    current_df = channel_info['current']
    prev_df = channel_info.get('previous')
    
    if current_df is None or len(current_df) == 0:
        return []
    
    # ãƒãƒ£ãƒãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã§ã¾ãšé›†è¨ˆ
    channel_summary = current_df.groupby('channel').agg({
        'sessions': 'sum',
        'users': 'sum',
        'purchases': 'sum',
        'revenue': 'sum',
    }).reset_index()
    
    # å‰æœŸé–“ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°æ¯”è¼ƒç”¨ã«é›†è¨ˆ
    prev_summary = None
    if prev_df is not None and len(prev_df) > 0:
        prev_summary = prev_df.groupby('channel').agg({
            'sessions': 'sum',
            'users': 'sum',
            'purchases': 'sum',
            'revenue': 'sum',
        }).reset_index()
        prev_summary = prev_summary.set_index('channel')
    
    # è©³ç´°ã‚½ãƒ¼ã‚¹ã‚’å–å¾—ï¼ˆãƒãƒ£ãƒãƒ«ã”ã¨ï¼‰
    source_details = {}
    for channel in current_df['channel'].unique():
        sources = current_df[current_df['channel'] == channel].groupby('source').agg({
            'sessions': 'sum',
            'users': 'sum',
            'purchases': 'sum',
            'revenue': 'sum',
        }).reset_index()
        sources = sources.sort_values('revenue', ascending=False).head(5)
        source_details[channel] = sources.to_dict('records')
    
    # çµæœã‚’æ•´å½¢
    results = []
    for _, row in channel_summary.iterrows():
        channel = row['channel']
        item = {
            'channel': channel,
            'channel_ja': translate_channel_name(channel),
            'sessions': int(row['sessions']),
            'users': int(row['users']),
            'purchases': int(row['purchases']),
            'revenue': float(row['revenue']),
            'cvr': round((row['purchases'] / row['sessions'] * 100) if row['sessions'] > 0 else 0, 2),
        }
        
        # å‰æœŸé–“ã¨ã®æ¯”è¼ƒ
        if prev_summary is not None and channel in prev_summary.index:
            prev = prev_summary.loc[channel]
            item['prev_sessions'] = int(prev['sessions'])
            item['prev_users'] = int(prev['users'])
            item['prev_purchases'] = int(prev['purchases'])
            item['prev_revenue'] = float(prev['revenue'])
            
            # ãƒ‡ãƒ«ã‚¿è¨ˆç®—
            item['delta_sessions'] = item['sessions'] - item['prev_sessions']
            item['delta_revenue'] = item['revenue'] - item['prev_revenue']
            item['delta_purchases'] = item['purchases'] - item['prev_purchases']
            
            # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆå¤‰åŒ–
            if item['prev_sessions'] > 0:
                item['delta_sessions_pct'] = round((item['sessions'] - item['prev_sessions']) / item['prev_sessions'] * 100, 1)
            else:
                item['delta_sessions_pct'] = 100 if item['sessions'] > 0 else 0
            if item['prev_revenue'] > 0:
                item['delta_revenue_pct'] = round((item['revenue'] - item['prev_revenue']) / item['prev_revenue'] * 100, 1)
            else:
                item['delta_revenue_pct'] = 100 if item['revenue'] > 0 else 0
            if item['prev_purchases'] > 0:
                item['delta_purchases_pct'] = round((item['purchases'] - item['prev_purchases']) / item['prev_purchases'] * 100, 1)
            else:
                item['delta_purchases_pct'] = 100 if item['purchases'] > 0 else 0
        else:
            item['prev_sessions'] = 0
            item['prev_revenue'] = 0
            item['prev_purchases'] = 0
            item['delta_sessions'] = item['sessions']
            item['delta_revenue'] = item['revenue']
            item['delta_purchases'] = item['purchases']
            item['delta_sessions_pct'] = 0
            item['delta_revenue_pct'] = 0
            item['delta_purchases_pct'] = 0
        
        # è©³ç´°ã‚½ãƒ¼ã‚¹ï¼ˆæ—¥æœ¬èªåŒ–ï¼‰
        item['sources'] = []
        if channel in source_details:
            for src in source_details[channel]:
                item['sources'].append({
                    'name': translate_source_name(src['source']),
                    'sessions': int(src['sessions']),
                    'purchases': int(src['purchases']),
                    'revenue': float(src['revenue']),
                })
        
        results.append(item)
    
    # å£²ä¸Šé †ã§ã‚½ãƒ¼ãƒˆ
    results.sort(key=lambda x: x['revenue'], reverse=True)
    return results


def get_pv_ranking(brand=None, limit=50):
    """PVï¼ˆé–²è¦§æ•°ï¼‰ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’å–å¾—ï¼ˆå•†å“åã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã€SKUè©³ç´°ä»˜ãï¼‰"""
    df = data_store['merged_data']
    if df is None:
        return []
    
    # ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿
    if brand and brand != 'all':
        df = df[df['brand'] == brand].copy()
    else:
        df = df.copy()
    
    # å•†å“åï¼ˆproduct_class_idï¼‰ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦é›†è¨ˆ
    if 'product_class_id' not in df.columns:
        return []
    
    # é–²è¦§æ•°ãŒ0ã‚ˆã‚Šå¤§ãã„å•†å“ã®ã¿ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—é›†è¨ˆå‰ï¼‰
    products_with_views = df[df['views'] > 0]['product_class_id'].unique()
    
    grouped = df.groupby('product_class_id').agg({
        'brand': 'first',
        'product_name': 'first',
        'image_url': 'first',
        'product_url': 'first',
        'views': 'first',  # GA4ã®PVã¯å•†å“åãƒ¬ãƒ™ãƒ«ã§åŒã˜å€¤
        'add_to_cart': 'sum',
        'purchases': 'sum',
        'revenue': 'sum',
        'total_stock': 'sum',
    }).reset_index()
    
    # PVãŒã‚ã‚‹å•†å“ã®ã¿
    grouped = grouped[grouped['product_class_id'].isin(products_with_views)]
    
    # CVRï¼ˆPVã«å¯¾ã™ã‚‹è³¼å…¥ç‡ï¼‰= è³¼å…¥æ•° / PV * 100
    grouped['cvr'] = (grouped['purchases'].astype(float) / grouped['views'].astype(float) * 100).fillna(0)
    grouped['purchase_rate'] = grouped['cvr']
    
    grouped = grouped.sort_values('views', ascending=False).head(limit)
    
    # SKUè©³ç´°ã‚’è¿½åŠ ï¼ˆå…ƒã®dfã‹ã‚‰å–å¾—ï¼‰
    result = []
    for _, row in grouped.iterrows():
        product = row.to_dict()
        # CVRã‚’ç¢ºå®Ÿã«floatã§ä¿æŒ
        product['cvr'] = float(product.get('cvr', 0) or 0)
        
        # ã“ã®product_class_idã«å±ã™ã‚‹å…¨SKUã‚’å–å¾—ï¼ˆå…ƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰
        skus = df[df['product_class_id'] == row['product_class_id']].copy()
        
        if len(skus) > 0:
            # å„SKUã®CVRã‚’è¨ˆç®—
            skus['cvr'] = skus.apply(
                lambda x: (float(x['purchases']) / float(x['views']) * 100) if float(x['views']) > 0 else 0.0, axis=1
            )
            
            # è³¼å…¥æ•°ã®å¤šã„é †ã«ã‚½ãƒ¼ãƒˆï¼ˆå„ªã‚Œã¦ã„ã‚‹é †ï¼‰
            skus = skus.sort_values('purchases', ascending=False)
            
            # SKUãƒ‡ãƒ¼ã‚¿ã‚’è¾æ›¸ãƒªã‚¹ãƒˆã«å¤‰æ›ï¼ˆãƒ‡ãƒ«ã‚¿æƒ…å ±ã‚‚å«ã‚€ï¼‰
            sku_list = []
            for _, s in skus.iterrows():
                sku_data = {
                    'color_name': str(s.get('color_name', '') or ''),
                    'color_tag': str(s.get('color_tag', '#888') or '#888'),
                    'size': str(s.get('size', '') or ''),
                    'views': int(s.get('views', 0) or 0),
                    'add_to_cart': int(s.get('add_to_cart', 0) or 0),
                    'purchases': int(s.get('purchases', 0) or 0),
                    'cvr': float(s.get('cvr', 0) or 0),
                    'total_stock': int(s.get('total_stock', 0) or 0),
                    # ãƒ‡ãƒ«ã‚¿æƒ…å ±
                    'delta_purchases': int(s.get('delta_purchases', 0) or 0),
                    'delta_purchases_pct': float(s.get('delta_purchases_pct', 0) or 0),
                    'delta_add_to_cart': int(s.get('delta_add_to_cart', 0) or 0),
                    'delta_cvr': float(s.get('delta_cvr', 0) or 0),
                    'prev_purchases': int(s.get('prev_purchases', 0) or 0),
                }
                sku_list.append(sku_data)
            product['skus'] = sku_list
        else:
            product['skus'] = []
        
        result.append(product)
    
    return result


def get_pv_ranking_by_brand(limit_per_brand=30):
    """ãƒ–ãƒ©ãƒ³ãƒ‰åˆ¥PVãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’å–å¾—"""
    df = data_store['merged_data']
    if df is None:
        return {}
    
    brands = df['brand'].dropna().unique().tolist()
    result = {}
    
    for brand in brands:
        result[brand] = get_pv_ranking(brand=brand, limit=limit_per_brand)
    
    return result


def get_grouped_products(df, sort_by='views', limit=50):
    """å•†å“ã‚’å‹ç•ªï¼ˆproduct_class_idï¼‰ã§ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°"""
    if df is None or len(df) == 0:
        return []
    
    # product_class_idã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦é›†è¨ˆ
    grouped = df.groupby('product_class_id').agg({
        'sku_id': 'count',  # SKUæ•°
        'brand': 'first',
        'product_name': 'first',
        'image_url': 'first',
        'product_url': 'first',
        'total_stock': 'sum',
        'views': 'sum',
        'add_to_cart': 'sum',
        'purchases': 'sum',
        'revenue': 'sum',
    }).reset_index()
    
    grouped.columns = ['product_class_id', 'sku_count', 'brand', 'product_name', 
                       'image_url', 'product_url', 'total_stock', 'views', 
                       'add_to_cart', 'purchases', 'revenue']
    
    # CVRè¨ˆç®—
    grouped['cvr'] = grouped.apply(
        lambda x: (x['purchases'] / x['views'] * 100) if x['views'] > 0 else 0, axis=1
    )
    
    # ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’å–å¾—
    grouped = grouped.sort_values(sort_by, ascending=False).head(limit)
    
    # å„ã‚°ãƒ«ãƒ¼ãƒ—ã®SKUè©³ç´°ã‚’å–å¾—
    result = []
    for _, row in grouped.iterrows():
        product = row.to_dict()
        # ã“ã®product_class_idã«å±ã™ã‚‹SKUã‚’å–å¾—
        skus = df[df['product_class_id'] == row['product_class_id']].sort_values(sort_by, ascending=False)
        product['skus'] = skus.to_dict('records')
        result.append(product)
    
    return result


def get_top_performers_grouped(brand=None, limit=20):
    """å£²ä¸Šä¸Šä½å•†å“ã‚’å–å¾—ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼‰"""
    df = data_store['merged_data']
    if df is None:
        return []
    
    filtered = df.copy()
    if brand and brand != 'all':
        filtered = filtered[filtered['brand'] == brand]
    
    if 'product_class_id' in filtered.columns:
        return get_grouped_products(filtered, 'revenue', limit)
    
    filtered = filtered.sort_values('revenue', ascending=False).head(limit)
    return filtered.to_dict('records')


@app.route('/login', methods=['GET', 'POST'])
def login():
    """ãƒ­ã‚°ã‚¤ãƒ³ç”»é¢"""
    if request.method == 'POST':
        password = request.form.get('password', '')
        result = check_password(password)
        
        if result:
            session['logged_in'] = True
            session['is_admin'] = result['is_admin']
            session['accessible_brands'] = result['brands']
            
            if result['is_admin']:
                flash('ç®¡ç†è€…ã¨ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã—ãŸ', 'success')
            else:
                brand_names = ', '.join(result['brands'])
                flash(f'{brand_names} ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã§ã™', 'success')
            
            return redirect(url_for('index'))
        else:
            flash('ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒé•ã„ã¾ã™', 'error')
    return render_template('login.html')


@app.route('/logout')
def logout():
    """ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ"""
    session.pop('logged_in', None)
    session.pop('is_admin', None)
    session.pop('accessible_brands', None)
    return redirect(url_for('login'))


@app.route('/')
@login_required
def index():
    """ãƒ¡ã‚¤ãƒ³ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰"""
    has_data = data_store['merged_data'] is not None
    brands = []
    summary = None
    pv_ranking_by_brand = {}
    analysis_period = None
    
    # ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ãªãƒ–ãƒ©ãƒ³ãƒ‰ã‚’å–å¾—
    accessible_brands = session.get('accessible_brands', BRANDS)
    is_admin = session.get('is_admin', False)
    
    if has_data:
        all_brands = data_store['merged_data']['brand'].dropna().unique().tolist()
        
        # ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ãªãƒ–ãƒ©ãƒ³ãƒ‰ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
        if is_admin:
            brands = all_brands
        else:
            brands = [b for b in all_brands if any(
                b.lower() == ab.lower() for ab in accessible_brands
            )]
        
        summary = get_brand_summary()
        # ã‚µãƒãƒªãƒ¼ã‚‚ãƒ•ã‚£ãƒ«ã‚¿
        if summary and not is_admin:
            summary = [s for s in summary if any(
                s['brand'].lower() == ab.lower() for ab in accessible_brands
            )]
        
        pv_ranking_by_brand = get_pv_ranking_by_brand(limit_per_brand=30)
        # PVãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚‚ãƒ•ã‚£ãƒ«ã‚¿
        if not is_admin:
            pv_ranking_by_brand = {k: v for k, v in pv_ranking_by_brand.items() 
                                   if any(k.lower() == ab.lower() for ab in accessible_brands)}
        
        analysis_period = get_analysis_period()
    
    # åˆ©ç”¨å¯èƒ½ãªæœŸé–“ã‚’ç¢ºèª
    available_periods = []
    for p in ['yesterday', 'weekly', '3days']:
        if data_store['periods_data'][p]['ga_sales']:
            available_periods.append(p)
    
    return render_template('index.html', 
                         has_data=has_data, 
                         brands=brands,
                         summary=summary,
                         pv_ranking_by_brand=pv_ranking_by_brand,
                         analysis_period=analysis_period,
                         is_admin=is_admin,
                         current_period=data_store['current_period'],
                         available_periods=available_periods)


@app.route('/switch-period/<period_type>')
@login_required
def switch_period(period_type):
    """æœŸé–“ã‚’åˆ‡ã‚Šæ›¿ãˆ"""
    valid_periods = ['yesterday', 'weekly', '3days']
    if period_type not in valid_periods:
        flash('ç„¡åŠ¹ãªæœŸé–“ã§ã™', 'error')
        return redirect(url_for('index'))
    
    # æœŸé–“ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
    if not data_store['periods_data'][period_type]['ga_sales']:
        flash(f'{period_type}ã®ãƒ‡ãƒ¼ã‚¿ãŒã¾ã å–å¾—ã•ã‚Œã¦ã„ã¾ã›ã‚“', 'error')
        return redirect(url_for('index'))
    
    # æœŸé–“ã‚’åˆ‡ã‚Šæ›¿ãˆ
    switch_period_data(period_type)
    
    period_names = {'yesterday': 'å‰æ—¥', 'weekly': 'é€±é–“', '3days': '3æ—¥é–“'}
    flash(f'{period_names.get(period_type, period_type)}ãƒ‡ãƒ¼ã‚¿ã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸ', 'success')
    return redirect(url_for('index'))


@app.route('/upload', methods=['GET', 'POST'])
@admin_required
def upload():
    """CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”»é¢"""
    if request.method == 'POST':
        # å•†å“ãƒã‚¹ã‚¿
        if 'product_csv' in request.files:
            file = request.files['product_csv']
            if file.filename:
                filename = secure_filename(file.filename)
                filepath = os.path.join(app.config['UPLOAD_FOLDER'], 'product_master.csv')
                file.save(filepath)
                try:
                    data_store['product_master'] = load_product_master(filepath)
                    # R2ã«ã‚‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
                    if is_r2_enabled():
                        upload_product_master(filepath)
                        flash(f'å•†å“ãƒã‚¹ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼†R2ã«ä¿å­˜ã—ã¾ã—ãŸï¼ˆ{len(data_store["product_master"])}ä»¶ï¼‰', 'success')
                    else:
                        flash(f'å•†å“ãƒã‚¹ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ{len(data_store["product_master"])}ä»¶ï¼‰', 'success')
                except Exception as e:
                    flash(f'å•†å“ãƒã‚¹ã‚¿ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}', 'error')
        
        # ãƒ–ãƒ©ãƒ³ãƒ‰åˆ¥GAå£²ä¸Š
        for brand in BRANDS:
            field_name = f'ga_csv_{brand}'
            if field_name in request.files:
                file = request.files[field_name]
                if file.filename:
                    filename = secure_filename(file.filename)
                    filepath = os.path.join(app.config['UPLOAD_FOLDER'], f'ga_sales_{brand}.csv')
                    file.save(filepath)
                    try:
                        ga_result = load_ga_sales(filepath)
                        data_store['ga_sales'][brand] = ga_result
                        period = ga_result['period']
                        period_str = ""
                        if period['start_date'] and period['end_date']:
                            period_str = f"ï¼ˆ{period['start_date'].strftime('%m/%d')}ã€œ{period['end_date'].strftime('%m/%d')}ï¼‰"
                        flash(f'{brand.upper()} GAå£²ä¸Šã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ{len(ga_result["data"])}ä»¶ï¼‰{period_str}', 'success')
                    except Exception as e:
                        flash(f'{brand.upper()} GAå£²ä¸Šã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}', 'error')
        
        # å•†å“ãƒã‚¹ã‚¿ã¨GAãƒ‡ãƒ¼ã‚¿ãŒæƒã£ãŸã‚‰åˆ†æå®Ÿè¡Œ
        if data_store['product_master'] is not None and data_store['ga_sales']:
            merge_and_analyze()
            flash('ãƒ‡ãƒ¼ã‚¿ã®çªåˆãƒ»åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼', 'success')
            return redirect(url_for('index'))
    
    # ç¾åœ¨ã®GAèª­ã¿è¾¼ã¿çŠ¶æ³ï¼ˆæœŸé–“æƒ…å ±å«ã‚€ï¼‰
    ga_status = {}
    for brand in BRANDS:
        if brand in data_store['ga_sales'] and data_store['ga_sales'][brand]:
            ga_info = data_store['ga_sales'][brand]
            ga_status[brand] = {
                'loaded': True,
                'count': len(ga_info['data']) if 'data' in ga_info else 0,
                'period': ga_info.get('period', {})
            }
        else:
            ga_status[brand] = {'loaded': False, 'count': 0, 'period': {}}
    
    # R2ã®å•†å“ãƒã‚¹ã‚¿æƒ…å ±
    r2_product_info = None
    if is_r2_enabled():
        r2_product_info = get_product_master_info()
    
    return render_template('upload.html',
                         has_product=data_store['product_master'] is not None,
                         ga_status=ga_status,
                         brands=BRANDS,
                         r2_enabled=is_r2_enabled(),
                         r2_product_info=r2_product_info,
                         ga4_api_enabled=is_ga4_configured())


@app.route('/sync-r2', methods=['POST'])
@login_required
def sync_r2():
    """R2ã‹ã‚‰å•†å“ãƒã‚¹ã‚¿ã‚’åŒæœŸ"""
    if not is_r2_enabled():
        flash('R2ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“', 'error')
        return redirect(url_for('upload'))
    
    try:
        df = download_product_master()
        if df is not None:
            data_store['product_master'] = process_product_master_df(df)
            flash(f'R2ã‹ã‚‰å•†å“ãƒã‚¹ã‚¿ã‚’åŒæœŸã—ã¾ã—ãŸï¼ˆ{len(data_store["product_master"])}ä»¶ï¼‰', 'success')
            
            # GAãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°å†åˆ†æ
            if data_store['ga_sales']:
                merge_and_analyze()
        else:
            flash('R2ã«å•†å“ãƒã‚¹ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“', 'error')
    except Exception as e:
        flash(f'R2åŒæœŸã‚¨ãƒ©ãƒ¼: {str(e)}', 'error')
    
    return redirect(url_for('upload'))


@app.route('/fetch-ga4', methods=['POST'])
@login_required
def fetch_ga4():
    """GA4 APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆå‰æœŸé–“ãƒ‡ãƒ¼ã‚¿ã‚‚åŒæ™‚å–å¾—ï¼‰"""
    if not is_ga4_configured():
        flash('GA4 APIãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“', 'error')
        return redirect(url_for('upload'))
    
    period_type = request.form.get('period_type', 'weekly')  # 'yesterday', 'weekly', or '3days'
    
    try:
        # ç¾åœ¨æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        results = fetch_all_brands_data(period_type)
        
        if not results:
            flash('GA4ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ', 'error')
            return redirect(url_for('upload'))
        
        # å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’data_storeã«ä¿å­˜ & R2ã«ã‚‚ä¿å­˜
        for brand, result in results.items():
            data_store['ga_sales'][brand] = result
            period = result['period']
            period_str = f"ï¼ˆ{period['start_date'].strftime('%m/%d')}ã€œ{period['end_date'].strftime('%m/%d')}ï¼‰"
            flash(f'{brand.upper()} GA4ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸï¼ˆ{len(result["data"])}ä»¶ï¼‰{period_str}', 'success')
            
            # R2ã«ä¿å­˜
            if save_ga4_data and is_r2_enabled():
                start_str = period['start_date'].strftime('%Y%m%d')
                end_str = period['end_date'].strftime('%Y%m%d')
                save_ga4_data(brand, result['data'], start_str, end_str)
        
        # å‰æœŸé–“ãƒ‡ãƒ¼ã‚¿ã‚‚å–å¾—ï¼ˆæ¯”è¼ƒç”¨ï¼‰
        from ga4_api import fetch_day_before_yesterday_data, fetch_previous_3days_data, fetch_previous_weekly_data, fetch_all_brands_channel_data
        
        for brand in results.keys():
            prev_result = None
            if period_type == 'yesterday':
                prev_result = fetch_day_before_yesterday_data(brand)
            elif period_type == '3days':
                prev_result = fetch_previous_3days_data(brand)
            elif period_type == 'weekly':
                prev_result = fetch_previous_weekly_data(brand)
            
            if prev_result:
                data_store['ga_sales_previous'][brand] = prev_result
                prev_period = prev_result['period']
                print(f"[OK] Fetched previous period data for {brand}: {len(prev_result['data'])} items")
        
        # ãƒãƒ£ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã‚‚å–å¾—
        try:
            channel_results = fetch_all_brands_channel_data(period_type)
            for brand, channel_df in channel_results.items():
                data_store['channel_data'][brand] = channel_df
                print(f"[OK] Fetched channel data for {brand}: {len(channel_df)} channels")
        except Exception as e:
            print(f"[WARN] Failed to fetch channel data: {e}")
        
        # å•†å“ãƒã‚¹ã‚¿ãŒã‚ã‚Œã°åˆ†æå®Ÿè¡Œ
        if data_store['product_master'] is not None:
            merge_and_analyze()
            flash('ãƒ‡ãƒ¼ã‚¿ã®çªåˆãƒ»åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼', 'success')
            return redirect(url_for('index'))
        
    except Exception as e:
        flash(f'GA4ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}', 'error')
    
    return redirect(url_for('upload'))


# ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æ›´æ–°ç”¨ã®ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆãƒˆãƒ¼ã‚¯ãƒ³
SCHEDULER_SECRET = os.environ.get('SCHEDULER_SECRET', 'default-scheduler-secret-change-me')

@app.route('/api/scheduled-update', methods=['POST'])
def scheduled_update():
    """
    Cloud Schedulerã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹è‡ªå‹•æ›´æ–°ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    æ¯æ—¥12:00 JSTã«å®Ÿè¡Œã•ã‚Œã€3æœŸé–“ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    """
    # ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã§èªè¨¼
    auth_header = request.headers.get('X-Scheduler-Secret', '')
    if auth_header != SCHEDULER_SECRET:
        return jsonify({'error': 'Unauthorized'}), 401
    
    if not is_ga4_configured():
        return jsonify({'error': 'GA4 API not configured'}), 500
    
    results_summary = {
        'timestamp': datetime.now().isoformat(),
        'periods': {},
        'success': True,
        'errors': []
    }
    
    try:
        from ga4_api import (
            fetch_all_brands_data, fetch_day_before_yesterday_data,
            fetch_previous_3days_data, fetch_previous_weekly_data,
            fetch_all_brands_channel_data
        )
        
        # 3æœŸé–“ã™ã¹ã¦ã‚’å–å¾—
        period_types = ['yesterday', 'weekly', '3days']
        
        for period_type in period_types:
            print(f"[SCHEDULER] Fetching {period_type} data...")
            results_summary['periods'][period_type] = {'brands': {}, 'success': True}
            
            try:
                # ç¾åœ¨æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                results = fetch_all_brands_data(period_type)
                
                if not results:
                    results_summary['periods'][period_type]['success'] = False
                    results_summary['errors'].append(f'Failed to fetch {period_type} data')
                    continue
                
                # ãƒ‡ãƒ¼ã‚¿ã‚’æœŸé–“åˆ¥ã‚¹ãƒˆã‚¢ã«ä¿å­˜
                for brand, result in results.items():
                    data_store['periods_data'][period_type]['ga_sales'][brand] = result
                    period = result['period']
                    
                    results_summary['periods'][period_type]['brands'][brand] = {
                        'items': len(result['data']),
                        'start': period['start_date'].strftime('%Y-%m-%d'),
                        'end': period['end_date'].strftime('%Y-%m-%d')
                    }
                    print(f"[SCHEDULER] {period_type}/{brand}: {len(result['data'])} items")
                
                # å‰æœŸé–“ãƒ‡ãƒ¼ã‚¿ã‚‚å–å¾—ï¼ˆæ¯”è¼ƒç”¨ï¼‰
                for brand in results.keys():
                    prev_result = None
                    if period_type == 'yesterday':
                        prev_result = fetch_day_before_yesterday_data(brand)
                    elif period_type == '3days':
                        prev_result = fetch_previous_3days_data(brand)
                    elif period_type == 'weekly':
                        prev_result = fetch_previous_weekly_data(brand)
                    
                    if prev_result:
                        data_store['periods_data'][period_type]['ga_sales_previous'][brand] = prev_result
                
                # ãƒãƒ£ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã‚‚å–å¾—
                try:
                    channel_results = fetch_all_brands_channel_data(period_type)
                    for brand, channel_df in channel_results.items():
                        data_store['periods_data'][period_type]['channel_data'][brand] = channel_df
                except Exception as e:
                    print(f"[SCHEDULER] Channel data error for {period_type}: {e}")
                
            except Exception as e:
                results_summary['periods'][period_type]['success'] = False
                results_summary['errors'].append(f'{period_type} error: {str(e)}')
                print(f"[SCHEDULER] Error fetching {period_type}: {e}")
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æœŸé–“ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¡ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ã«ã‚³ãƒ”ãƒ¼
        default_period = 'yesterday'
        switch_period_data(default_period)
        
        # å•†å“ãƒã‚¹ã‚¿ãŒã‚ã‚Œã°å…¨æœŸé–“ã®åˆ†æã‚’å®Ÿè¡Œ
        if data_store['product_master'] is not None:
            for period_type in period_types:
                merge_and_analyze_for_period(period_type)
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¡ã‚¤ãƒ³ã«ã‚»ãƒƒãƒˆ
            switch_period_data(default_period)
            print("[SCHEDULER] Data merge completed for all periods")
        
        print(f"[SCHEDULER] Scheduled update completed successfully for all periods")
        return jsonify(results_summary), 200
        
    except Exception as e:
        results_summary['success'] = False
        results_summary['errors'].append(str(e))
        print(f"[SCHEDULER] Error: {e}")
        return jsonify(results_summary), 500


@app.route('/brand/<brand_name>')
@login_required
def brand_detail(brand_name):
    """ãƒ–ãƒ©ãƒ³ãƒ‰åˆ¥è©³ç´°"""
    if data_store['merged_data'] is None:
        flash('ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ç®¡ç†è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚', 'error')
        return redirect(url_for('index'))
    
    # ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãƒã‚§ãƒƒã‚¯
    if not can_access_brand(brand_name):
        flash(f'{brand_name} ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“', 'error')
        return redirect(url_for('index'))
    
    brand = brand_name if brand_name != 'all' else None
    
    problem_products = get_problem_products(brand)
    opportunity_products = get_opportunity_products(brand)
    top_products = get_top_performers(brand)
    pv_ranking = get_pv_ranking(brand, limit=50)
    anomalies = get_anomalies(brand=brand, limit=10)
    
    # ãƒãƒ£ãƒãƒ«ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆæ•´å½¢æ¸ˆã¿ï¼‰
    channel_data = []
    brand_key = brand.lower() if brand else None
    if brand_key and brand_key in data_store.get('channel_data', {}):
        channel_info = data_store['channel_data'][brand_key]
        if channel_info:
            channel_data = process_channel_data(channel_info)
    
    # ãƒ–ãƒ©ãƒ³ãƒ‰çµ±è¨ˆ
    df = data_store['merged_data']
    if brand:
        brand_df = df[df['brand'] == brand]
    else:
        brand_df = df
    
    stats = {
        'total_sku': len(brand_df),
        'total_stock': int(brand_df['total_stock'].sum()),
        'total_revenue': float(brand_df['revenue'].sum()),
        'total_views': int(brand_df['views'].sum()),
        'avg_cvr': float(brand_df['cvr'].mean()),
        'problem_count': int(brand_df['is_problem'].sum()),
        'opportunity_count': int(brand_df['is_opportunity'].sum()),
    }
    
    all_brands = df['brand'].dropna().unique().tolist()
    analysis_period = get_analysis_period()
    
    # ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ãªãƒ–ãƒ©ãƒ³ãƒ‰ã®ã¿è¡¨ç¤º
    is_admin = session.get('is_admin', False)
    accessible_brands = session.get('accessible_brands', BRANDS)
    if is_admin:
        brands = all_brands
    else:
        brands = [b for b in all_brands if any(
            b.lower() == ab.lower() for ab in accessible_brands
        )]
    
    return render_template('brand_detail.html',
                         brand_name=brand_name,
                         stats=stats,
                         problem_products=problem_products,
                         opportunity_products=opportunity_products,
                         top_products=top_products,
                         pv_ranking=pv_ranking,
                         anomalies=anomalies,
                         channel_data=channel_data,
                         brands=brands,
                         analysis_period=analysis_period,
                         is_admin=is_admin)


@app.route('/api/products')
def api_products():
    """å•†å“ãƒ‡ãƒ¼ã‚¿API"""
    if data_store['merged_data'] is None:
        return jsonify([])
    
    brand = request.args.get('brand', 'all')
    category = request.args.get('category', 'all')  # problem, opportunity, top, pv
    limit = int(request.args.get('limit', 50))
    
    if category == 'problem':
        products = get_problem_products(brand, limit)
    elif category == 'opportunity':
        products = get_opportunity_products(brand, limit)
    elif category == 'pv':
        products = get_pv_ranking(brand, limit)
    else:
        products = get_top_performers(brand, limit)
    
    return jsonify(products)


@app.route('/admin/passwords', methods=['GET', 'POST'])
@admin_required
def admin_passwords():
    """ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ç®¡ç†ç”»é¢ï¼ˆç®¡ç†è€…å°‚ç”¨ï¼‰"""
    global password_cache
    
    if request.method == 'POST':
        action = request.form.get('action')
        
        if action == 'update_admin':
            new_password = request.form.get('admin_password', '').strip()
            if new_password and len(new_password) >= 4:
                update_password('admin', new_password=new_password)
                flash('ç®¡ç†è€…ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’æ›´æ–°ã—ã¾ã—ãŸ', 'success')
            else:
                flash('ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã¯4æ–‡å­—ä»¥ä¸Šã§å…¥åŠ›ã—ã¦ãã ã•ã„', 'error')
        
        elif action == 'update_brand':
            brand_key = request.form.get('brand_key', '').lower()
            new_password = request.form.get('brand_password', '').strip()
            if brand_key and new_password and len(new_password) >= 4:
                update_password('brand', brand_key=brand_key, new_password=new_password)
                flash(f'{brand_key} ã®ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’æ›´æ–°ã—ã¾ã—ãŸ', 'success')
            else:
                flash('ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã¯4æ–‡å­—ä»¥ä¸Šã§å…¥åŠ›ã—ã¦ãã ã•ã„', 'error')
        
        return redirect(url_for('admin_passwords'))
    
    # ç¾åœ¨ã®ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰è¨­å®šã‚’è¡¨ç¤º
    return render_template('admin_passwords.html',
                         brands=BRANDS,
                         password_cache=password_cache)


def init_from_r2():
    """èµ·å‹•æ™‚ã«R2ã‹ã‚‰æœ€æ–°ã®å•†å“ãƒã‚¹ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    if not is_r2_enabled():
        print("[WARN] R2 is not enabled. Set R2 environment variables to enable.")
        return False
    
    try:
        print("[INFO] Loading product master from R2...")
        df = download_product_master()
        if df is not None and len(df) > 0:
            data_store['product_master'] = process_product_master_df(df)
            data_store['product_master_info'] = get_product_master_info()
            print(f"[OK] Loaded {len(data_store['product_master'])} products from R2")
            
            # GA4ãƒ‡ãƒ¼ã‚¿ã‚‚èª­ã¿è¾¼ã¿
            if get_latest_ga4_data:
                print("[INFO] Loading GA4 data from R2...")
                for brand in BRANDS:
                    ga4_data = get_latest_ga4_data(brand)
                    if ga4_data:
                        from datetime import datetime
                        start_date = datetime.strptime(ga4_data['start_date'], '%Y%m%d') if ga4_data['start_date'] else None
                        end_date = datetime.strptime(ga4_data['end_date'], '%Y%m%d') if ga4_data['end_date'] else None
                        data_store['ga_sales'][brand] = {
                            'data': ga4_data['df'],
                            'period': {
                                'start_date': start_date,
                                'end_date': end_date,
                                'period_type': 'daily' if start_date == end_date else 'custom'
                            }
                        }
                        print(f"  âœ… {brand}: {len(ga4_data['df'])} rows")
                
                # å•†å“ãƒã‚¹ã‚¿ã¨GA4ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°åˆ†æå®Ÿè¡Œ
                if any(data_store['ga_sales'].values()):
                    merge_and_analyze()
                    print("[OK] Auto-merged data on startup")
            
            return True
        else:
            print("[WARN] No product master found in R2")
            return False
    except Exception as e:
        print(f"[ERROR] Error loading from R2: {e}")
        return False


# ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«åˆæœŸåŒ–
with app.app_context():
    init_passwords()  # ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰åˆæœŸåŒ–
    init_from_r2()    # R2ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿


if __name__ == '__main__':
    # é–‹ç™ºæ™‚ã¯debug=Trueã ãŒã€ç’°å¢ƒå¤‰æ•°ãŒæ¶ˆãˆã‚‹å•é¡ŒãŒã‚ã‚‹ã®ã§use_reloader=Falseã«
    app.run(debug=True, port=8080, host='0.0.0.0', use_reloader=False)
